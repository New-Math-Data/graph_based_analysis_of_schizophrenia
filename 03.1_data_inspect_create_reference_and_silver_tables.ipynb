{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4702c3bc-c7e7-44f4-9e63-57770c1d5c6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Overview - Data Inspection, Create Reference Channel and Create Silver Tables \n",
    "\n",
    "While EEG recordings can be made without a reference electrode, the use of a reference electrode is essential for accurate and meaningful EEG analysis. It helps in grounding the electrical potentials, canceling out common-mode noise, facilitating signal comparison, and enabling various analytical techniques. The choice of reference scheme should be made based on the experimental requirements and analytical considerations.\n",
    "\n",
    "##### In this notebook we will:\n",
    "  * Contrast the different techniques used in EEG data analysis to re-reference the data.\n",
    "    * Reference Electrode Standardization Technique (REST) Method\n",
    "      - REST is a method used in electrochemistry to ensure that measurements taken with different reference electrodes are comparable.  \n",
    "    * Average Reference Method\n",
    "      - In this method, you calculate the average signal across all electrodes and subtract this average from each electrode. This method assumes that the average potential of all electrodes represents a good approximation of zero potential.\n",
    "    * Current Source Density (CSD)\n",
    "  * Examine statistical metrics utilizing Databricks' integrated command `describe` for potential future data manipulation.\n",
    "    * `describe`: Provides statistics including count, mean, standard deviation, minimum, and maximum.\n",
    "    * Use the SQL table column functionality, `display` DataFrame, to look for missing or null values in the data that will cause outliers or skew the data, zero values are expected with EEG data so no need to wrangle them.\n",
    "  * Create Silver Layer Table\n",
    "\n",
    "##### This notebook we will create the REST Reference Method and respective tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fccd0145-cd1e-4bae-9988-c01d1e1d583f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Retrieve data from Bronze Tables\n",
    "###### To save on resources, we will only work with one Control Patient and one Study Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18079be2-e617-42eb-a76e-e3e1817c64d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_bronze_control = spark.sql(\"\"\"SELECT * FROM main.solution_accelerator.eeg_data_bronze_control WHERE patient_id = 'h13' ORDER BY index_id ASC\"\"\")\n",
    "\n",
    "df_bronze_study = spark.sql(\"\"\"SELECT * FROM main.solution_accelerator.eeg_data_bronze_study WHERE patient_id = 's11' ORDER BY index_id ASC\"\"\")\n",
    "\n",
    "# Inspect the DataFrames\n",
    "display(df_bronze_control)\n",
    "display(df_bronze_study)\n",
    "\n",
    "df_bronze_control.orderBy(col(\"patient_id\"), col(\"index_id\").asc()).show()\n",
    "df_bronze_study.orderBy(col(\"patient_id\"), col(\"index_id\").asc()).show()\n",
    "\n",
    "# Union the PySpark DataFrames\n",
    "df_bronze_patients = df_bronze_control.union(df_bronze_study)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b42de4f-6a7f-4def-a118-82b07341f9ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Verify number of rows in DataFrame equals the original raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe6b40b-91e4-4774-af82-6eb0e836c198",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the result\n",
    "# display(df_bronze_patients)\n",
    "# df_bronze_patients.groupby('patient_id').count().show()\n",
    "# display(df_bronze_patients.groupBy('patient_id').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2dc13e8-6f9c-40a7-b334-8b37c1cd05e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Convert PySpark Dataframe to Pandas Dataframe\n",
    "###### Use the SQL table column functionality, `display` DataFrame, to look for missing or null values in the data that will cause outliers or skew the data, zero values are expected with EEG data so no need to wrangle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76e9f87-9f6c-4933-ad31-2eed62fa3bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert our two PySpark Dataframes to Pandas Dataframes\n",
    "# display(df_bronze_patients.head())\n",
    "\n",
    "df_patients = df_bronze_patients.toPandas().sort_values(by=['time'], ascending=True)\n",
    "\n",
    "# Use the SQL table function `display`, click on the column filtering menu located on the output table, to look for missing or null values in the data that will cause outliers or skew the data, zero values are expected with EEG data so no need to wrangle them.\n",
    "display(df_patients)\n",
    "# display(df_patients.groupby('patient_id').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa1cc4b8-3cc5-4cc4-9940-3e2bd691825b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Contrast the different techniques used in EEG data analysis to re-reference the data.\n",
    "\n",
    "Each patient has distinct noise and artifact frequencies that are not part of the usable data and must be identified and filtered out.\n",
    "\n",
    "Contrast the EEG reference points generated by employing multiple methods to determine the perferred EEG reference point for our specific data and use case.\n",
    "\n",
    "Both the REST and Average Reference (AR) methods are used in EEG (Electroencephalography) to mitigate common noise sources and spatial biases. However, they differ in their approach to reference point selection and signal processing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f268cb3b-f408-4259-8a5f-397e9a7bab3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper library with many built-in functions\n",
    "%pip install mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df63df56-37ff-4c9f-b19a-50fc172d3728",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Let's make a Databricks method to convert PySpark DataFrames to MNE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00dfe24b-a3a1-463c-85ac-09fce35397d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "# Sampling rate in Hz\n",
    "sfreq = 250\n",
    "\n",
    "# Get channel names , extract patient_id column\n",
    "ch_names = [c for c in df_patients.columns if c not in ['patient_id', 'index_id', 'time']]\n",
    "print(\"CHANNELS::\", ch_names)\n",
    "print(f\"Length ch_names:::{len(ch_names)}\")\n",
    "\n",
    "# Extract patient_id column\n",
    "pt_names = list(df_patients['patient_id'].unique())\n",
    "print(f\"patient_ID:::{pt_names}\")\n",
    "\n",
    "mne_raw_all = {}\n",
    "\n",
    "for pt in pt_names:\n",
    "    print(\"PATIENT_ID::\", pt)\n",
    "    # Create an info structure needed by MNE\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "    df_pt_data = df_patients.loc[df_patients['patient_id'] == pt]\n",
    "    df_pt_data = df_pt_data.drop(columns=['patient_id', 'time', 'index_id'])\n",
    "    # print(\"LEN::\", len(df_pt_data.index))\n",
    "\n",
    "    # Convert Pandas Dataframe to Numpy Array for each patient\n",
    "    np_pt_data = df_pt_data.to_numpy() \n",
    "\n",
    "    # Create the MNE Raw object\n",
    "    mne_raw_pt = mne.io.RawArray(np_pt_data.T, info)\n",
    "\n",
    "    # Data is loaded and represented in microvolts (µV)\n",
    "    print(f\"Original data voltage changed by MNE:::{mne_raw_pt.info['chs'][0]['cal']}\")\n",
    "    # The mne raw data object gives us time, assess it as `data, times = raw[:]` \n",
    "     \n",
    "    # Channel mapping\n",
    "    mne_raw_all[pt] = mne_raw_pt.set_montage('standard_1020')\n",
    "  \n",
    "    # Plot the data so we can compare graphs to reference methods later\n",
    "    print(\"Patient_ID::\", pt)\n",
    "    mne_raw_all[pt].plot(start=10, duration=100, color='black', n_channels=19, title='Raw Data Patient:::{pt}', show=True, scalings=dict(eeg=50), bad_color='red')\n",
    "    print(\"Patient_ID::\", pt)\n",
    "    mne_raw_all[pt].plot_sensors(ch_type=\"eeg\")\n",
    "    print(\"Patient_ID::\", pt)\n",
    "    spectrum = mne_raw_all[pt].compute_psd().plot(average=True, picks=\"data\", exclude=\"bads\", amplitude=False)\n",
    "\n",
    "# Now we have our MNE Raw objects and are ready for further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edaad2f-5a08-4f77-9657-3476b2285393",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(np_pt_data) \n",
    "# print(np_pt_data.T)\n",
    "for d in mne_raw_all[pt]:\n",
    "    print(d)\n",
    "\n",
    "#display(mne_raw_all[pt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b289b5-2fc7-484f-8c1a-a30d19b71c91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### REST Method\n",
    "Reference Electrode Standardization Technique infinity reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01dd0f8-f573-4109-998e-e558808bdcbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "####### CALCULATING THE REFERENCE ELECTRODE USING THE REST METHOD #######\n",
    "\n",
    "# Extract patient_id column\n",
    "pt_names = list(df_patients['patient_id'].unique())\n",
    "# print(f\"patient_names:::{pt_names}\")\n",
    "# print(f\"mne_raw_all{mne_raw_all.keys()}\")\n",
    "\n",
    "mne_rest_all = {}\n",
    "\n",
    "# Calculate the average signal across all channels for each patient\n",
    "for pt in pt_names:\n",
    "    mne_raw_pt = mne_raw_all[pt]\n",
    "    # print(f\"type mne_raw_pt:::{type(mne_raw_pt)}\")\n",
    "    if isinstance(mne_raw_pt, mne.io.RawArray):\n",
    "        # Apply REST Method\n",
    "        # mne_raw_pt.del_proj()  # remove our average reference projector first\n",
    "        sphere = mne.make_sphere_model(\"auto\", \"auto\", mne_raw_pt.info)\n",
    "        src = mne.setup_volume_source_space(sphere=sphere, exclude=30.0, pos=15.0)\n",
    "        forward = mne.make_forward_solution(mne_raw_pt.info, trans=None, src=src, bem=sphere)\n",
    "        raw_rest = mne_raw_pt.copy().set_eeg_reference(\"REST\", forward=forward)\n",
    "        # print(f\"type raw_rest:::{type(raw_rest)}\")\n",
    "        mne_rest_all[pt] = raw_rest\n",
    "        # Plot EEG data\n",
    "        print(f\"Patient_ID:::{pt}\")\n",
    "        raw_rest.plot(scalings=dict(eeg=50), title='EEG Data {pt}', n_channels=19, start=10, duration=100)\n",
    "\n",
    "        # Show the plot\n",
    "        # plt.show()\n",
    "        # for title, _raw in zip([\"Original\", \"REST (∞)\"], [mne_raw_pt, raw_rest]):\n",
    "        #     with mne.viz.use_browser_backend(\"matplotlib\"):\n",
    "        #         fig = _raw.plot(n_channels=len(mne_raw_pt), start=10, scalings=dict(eeg=50))\n",
    "        #     print(f\"Patient_ID:::{pt}\")\n",
    "        #     fig.subplots_adjust(top=0.9)\n",
    "        #     fig.suptitle(f\"{title} reference\", size=\"xx-large\", weight=\"bold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1103336f-c7c7-4be3-b0ab-f40225362bd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for v in mne_rest_all[pt]:\n",
    "#     print(v)\n",
    "print(mne_rest_all.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7301c36-d436-42fc-92fc-c092f7c29877",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Dropping the table because we may have updated the Dataframe\n",
    "\n",
    "DROP TABLE IF EXISTS main.solution_accelerator.eeg_rest_ref_silver;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb56d757-b0ca-43f7-9d32-779180072146",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### `Export` REST Data and Create Silver Layer Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa100bda-42cc-4fa8-a4d3-a89c671d1539",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export REST Data to Silver Table\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "created = False\n",
    "\n",
    "for pt in mne_rest_all:\n",
    "    print(pt)\n",
    "    mne_rest_pt = mne_rest_all[pt]\n",
    "    # Check the updated scaling factor for the first channel\n",
    "    # print(f\"TYPE::{type(mne_rest_pt)}\")\n",
    "    df_rest_pd = mne_rest_pt.to_data_frame(picks=[\"eeg\"])\n",
    "    # print(f\"PT::{pt} LEN:{len(df_rest_pd.index)}\")\n",
    "    # Add a new index column. We do this so the data goes in, in order for the sine waves\n",
    "    df_rest_pd['index_id'] = df_rest_pd.index\n",
    "    df_rest_spark = spark.createDataFrame(df_rest_pd)\n",
    "    df_rest_spark = df_rest_spark.withColumn('patient_id', lit(pt))\n",
    "    # Establish a persistent delta table by converting the Spark DataFrames into a Delta Table\n",
    "    if not created:\n",
    "        # Replace any previously existing table (\"overwrite\") and register the Spark DataFrames as a Delta table in Catalog\n",
    "        print(\"CREATE\")\n",
    "        df_rest_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.solution_accelerator.eeg_rest_ref_silver\")\n",
    "        created = True\n",
    "    else:\n",
    "        print(\"APPEND\")\n",
    "        df_rest_spark.write.format(\"delta\").mode(\"append\").saveAsTable(\"main.solution_accelerator.eeg_rest_ref_silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65956472-b437-475c-ad04-2034a9d9c0cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Examine statistical metrics utilizing Databricks' integrated command `describe` for potential future data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd115c19-e534-4383-a623-2f364db2fc1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract patient_id column\n",
    "pt_names = list(df_patients['patient_id'].unique())\n",
    "# print(f\"patient_names:::{pt_names}\")\n",
    "\n",
    "for pt in pt_names:\n",
    "  print(\"PATIENT_ID::\", pt)\n",
    "  mne_rest_all[pt].describe()\n",
    "  df_rest_spark.describe()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "ddb5a3b4-a1e8-4a54-aa9a-887437b11eba",
     "origId": 3554137769111117,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3554137769111111,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03.1_data_inspect_create_reference_and_silver_tables",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
