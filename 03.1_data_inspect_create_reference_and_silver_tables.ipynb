{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4702c3bc-c7e7-44f4-9e63-57770c1d5c6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Overview - Data Inspection, Create Reference Channel and Create Silver Tables \n",
    "\n",
    "While EEG recordings can be made without a reference electrode, the use of a reference electrode is essential for accurate and meaningful EEG analysis. It helps in grounding the electrical potentials, canceling out common-mode noise, facilitating signal comparison, and enabling various analytical techniques. The choice of reference scheme should be made based on the experimental requirements and analytical considerations.\n",
    "\n",
    "##### In this notebook we will:\n",
    "  * Contrast the different techniques used in EEG data analysis to re-reference the data.\n",
    "    * Reference Electrode Standardization Technique (REST) Method\n",
    "      - REST is a method used in electrochemistry to ensure that measurements taken with different reference electrodes are comparable.  \n",
    "    * Average Reference Method\n",
    "      - In this method, you calculate the average signal across all electrodes and subtract this average from each electrode. This method assumes that the average potential of all electrodes represents a good approximation of zero potential.\n",
    "    * CSD \n",
    "  * Examine statistical metrics utilizing Databricks' integrated commands `describe` and `summary` for potential future data manipulation.\n",
    "    * `describe`: Provides statistics including count, mean, standard deviation, minimum, and maximum.\n",
    "    * `summary`: describe + interquartile range (IQR)\n",
    "    * Look for missing or null values in the data that will cause outliers or skew the data, zero values are expected with EEG data so no need to wrangle them.\n",
    "  * Create Silver Layer Table\n",
    "\n",
    "##### This notebook we will create the REST Reference Method and respective tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fccd0145-cd1e-4bae-9988-c01d1e1d583f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Retrieve data from Bronze Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18079be2-e617-42eb-a76e-e3e1817c64d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_bronze_control = spark.sql(\"\"\"SELECT * FROM main.solution_accelerator.eeg_data_bronze_control WHERE patient_id in ('s11', 'h13') ORDER BY index_id ASC\"\"\")\n",
    "df_bronze_study = spark.sql(\"\"\"SELECT * FROM main.solution_accelerator.eeg_data_bronze_study WHERE patient_id in ('s11', 'h13') ORDER BY index_id ASC\"\"\")\n",
    "\n",
    "# Inspect the DataFrames\n",
    "# display(df_bronze_control)\n",
    "# display(df_bronze_study)\n",
    "\n",
    "df_bronze_control.orderBy(col(\"patient_id\"), col(\"index_id\").asc()).show()\n",
    "df_bronze_study.orderBy(col(\"patient_id\"), col(\"index_id\").asc()).show()\n",
    "\n",
    "# Union the PySpark DataFrames\n",
    "df_bronze_patients = df_bronze_control.union(df_bronze_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b42de4f-6a7f-4def-a118-82b07341f9ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Verify number of rows in DataFrame equals the original raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe6b40b-91e4-4774-af82-6eb0e836c198",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the result\n",
    "# display(df_bronze_patients)\n",
    "# df_bronze_patients.groupby('patient_id').count().show()\n",
    "# display(df_bronze_patients.groupBy('patient_id').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2dc13e8-6f9c-40a7-b334-8b37c1cd05e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Convert PySpark Dataframe to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76e9f87-9f6c-4933-ad31-2eed62fa3bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": {
        "addedWidgets": {},
        "arguments": {},
        "datasetInfos": [],
        "jupyterProps": null,
        "metadata": {},
        "removedWidgets": [],
        "sqlProps": null,
        "stackFrames": [
         "com.databricks.backend.common.rpc.CommandSkippedException",
         "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:105)",
         "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)",
         "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
         "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)",
         "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:718)",
         "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:437)",
         "\tat scala.Option.getOrElse(Option.scala:189)",
         "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:437)",
         "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1266)",
         "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:983)",
         "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
         "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
         "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
         "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
         "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
         "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
         "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
         "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
         "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)",
         "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
         "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
         "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)",
         "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
         "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
         "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)",
         "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
         "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
         "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)",
         "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:944)",
         "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:682)",
         "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:708)",
         "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
         "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
         "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
         "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
         "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
         "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
         "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
         "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
         "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
         "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
         "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
         "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
         "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
         "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
         "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:707)",
         "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:762)",
         "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:555)",
         "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
         "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
         "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
         "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
         "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
         "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
         "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
         "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
         "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
         "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
         "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
         "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
         "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
         "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
         "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
         "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
         "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
         "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
         "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
         "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
         "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
         "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
         "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
         "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)",
         "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)",
         "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)",
         "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)",
         "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)",
         "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)",
         "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
         "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
         "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
         "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
         "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
         "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)",
         "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)",
         "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)",
         "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)",
         "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)",
         "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)",
         "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)",
         "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
         "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
         "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
         "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
         "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
         "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
         "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
         "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
         "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
         "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
         "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
         "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
         "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
         "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
         "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
         "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
         "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
         "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
         "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
         "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
         "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
         "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
         "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
         "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
         "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
         "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
         "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)",
         "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
         "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)",
         "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
         "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)",
         "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)",
         "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
         "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)",
         "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)",
         "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)",
         "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)",
         "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
         "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
         "\tat java.lang.Thread.run(Thread.java:750)"
        ],
        "type": "baseError"
       },
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBDb252ZXJ0IG91ciB0d28gUHlTcGFyayBEYXRhZnJhbWVzIHRvIFBhbmRhcyBEYXRhZnJhbWVzCmRpc3BsYXkoZGZfYnJvbnplX3BhdGllbnRzLmhlYWQoKSkKCmRmX3BhdGllbnRzID0gZGZfYnJvbnplX3BhdGllbnRzLnRvUGFuZGFzKCkuc29ydF92YWx1ZXMoYnk9WydpbmRleF9pZCddLCBhc2NlbmRpbmc9VHJ1ZSkKCmRpc3BsYXkoZGZfcGF0aWVudHMpCmRpc3BsYXkoZGZfcGF0aWVudHMuZ3JvdXBieSgncGF0aWVudF9pZCcpLmNvdW50KCkp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 1:\n      # run summarize\n      if type(__data_summary_dfs[1]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[1].to_spark())\n      elif type(__data_summary_dfs[1]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[1]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[1])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "dbe1e1c9-0591-4e76-84d2-189fba58b141",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 0,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 1,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert our two PySpark Dataframes to Pandas Dataframes\n",
    "# display(df_bronze_patients.head())\n",
    "\n",
    "df_patients = df_bronze_patients.toPandas().sort_values(by=['index_id'], ascending=True)\n",
    "\n",
    "# display(df_patients)\n",
    "# display(df_patients.groupby('patient_id').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa1cc4b8-3cc5-4cc4-9940-3e2bd691825b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Contrast the different techniques used in EEG data analysis to re-reference the data.\n",
    "\n",
    "Each patient has distinct noise and artifact frequencies that are not part of the usable data and must be identified and filtered out.\n",
    "\n",
    "Contrast the EEG reference points generated by employing multiple methods to determine the perferred EEG reference point for our specific data and use case.\n",
    "\n",
    "Both the REST and Average Reference (AR) methods are used in EEG (Electroencephalography) to mitigate common noise sources and spatial biases. However, they differ in their approach to reference point selection and signal processing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f268cb3b-f408-4259-8a5f-397e9a7bab3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper library with many built-in functions\n",
    "%pip install mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df63df56-37ff-4c9f-b19a-50fc172d3728",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Let's make a Databricks method to convert PySpark DataFrames to MNE Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00dfe24b-a3a1-463c-85ac-09fce35397d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "# Sampling rate in Hz\n",
    "sfreq = 250\n",
    "\n",
    "# Get channel names , extract patient_id column\n",
    "ch_names = [c for c in df_patients.head() if c != 'patient_id' and c != 'index_id']\n",
    "# print(f\"ch_names:::{ch_names}\")\n",
    "\n",
    "# Extract patient_id column\n",
    "pt_names = list(df_patients['patient_id'].unique())\n",
    "# print(f\"patient_names:::{pt_names}\")\n",
    "\n",
    "mne_raw_all = {}\n",
    "\n",
    "for pt in pt_names:\n",
    "    print(\"PATIENT_NAME::\", pt)\n",
    "    # Create an info structure needed by MNE\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "    df_pt_data = df_patients.loc[df_patients['patient_id'] == pt]\n",
    "    df_pt_data = df_pt_data.drop(columns=['patient_id', 'index_id'])\n",
    "    # print(\"LEN::\", len(df_pt_data.index))\n",
    "    # Convert Pandas Dataframe to Numpy Array for each patient\n",
    "    np_pt_data = df_pt_data.to_numpy() \n",
    "    # Create the MNE Raw object\n",
    "    mne_raw_pt = mne.io.RawArray(np_pt_data.T, info)\n",
    "    # The mne raw data object gives us time, assess it as `data, times = raw[:]`  \n",
    "    # Channel mapping\n",
    "    mne_raw_pt_w_montage = mne_raw_pt.set_montage('standard_1020')\n",
    "    mne_raw_all[pt] = mne_raw_pt_w_montage\n",
    "    \n",
    "    # Plot the data so we can compare graphs to reference methods later\n",
    "    mne_raw_pt_w_montage.plot(scalings=dict(eeg=50), start=150, duration=100)\n",
    "    mne_raw_pt_w_montage.plot_sensors(ch_type=\"eeg\")\n",
    "    spectrum = mne_raw_pt_w_montage.compute_psd().plot(average=True, picks=\"data\", exclude=\"bads\", amplitude=False)\n",
    "    \n",
    "# Now we have our MNE Raw objects and are ready for further analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b289b5-2fc7-484f-8c1a-a30d19b71c91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### REST Method\n",
    "Reference Electrode Standardization Technique infinity reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01dd0f8-f573-4109-998e-e558808bdcbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "####### CALCULATING THE REFERENCE ELECTRODE USING THE REST METHOD #######\n",
    "\n",
    "# Extract patient_id column\n",
    "pt_names = list(df_patients['patient_id'].unique())\n",
    "# print(f\"patient_names:::{pt_names}\")\n",
    "# print(f\"mne_raw_all{mne_raw_all.keys()}\")\n",
    "\n",
    "mne_rest_all = {}\n",
    "\n",
    "# Calculate the average signal across all channels for each patient\n",
    "for pt in pt_names:\n",
    "    mne_raw_pt = mne_raw_all[pt]\n",
    "    # print(f\"type mne_raw_pt:::{type(mne_raw_pt)}\")\n",
    "    if isinstance(mne_raw_pt, mne.io.RawArray):\n",
    "        # Apply REST Method\n",
    "        mne_raw_pt.del_proj()  # remove our average reference projector first\n",
    "        sphere = mne.make_sphere_model(\"auto\", \"auto\", mne_raw_pt.info)\n",
    "        src = mne.setup_volume_source_space(sphere=sphere, exclude=30.0, pos=15.0)\n",
    "        forward = mne.make_forward_solution(mne_raw_pt.info, trans=None, src=src, bem=sphere)\n",
    "        raw_rest = mne_raw_pt.copy().set_eeg_reference(\"REST\", forward=forward)\n",
    "        # print(f\"type raw_rest:::{type(raw_rest)}\")\n",
    "        mne_rest_all[pt] = raw_rest\n",
    "        for title, _raw in zip([\"Original\", \"REST (∞)\"], [mne_raw_pt, raw_rest]):\n",
    "            with mne.viz.use_browser_backend(\"matplotlib\"):\n",
    "                fig = _raw.plot(n_channels=len(mne_raw_pt), scalings=dict(eeg=50))\n",
    "            # make room for title\n",
    "            fig.subplots_adjust(top=0.9)\n",
    "            fig.suptitle(f\"{title} reference\", size=\"xx-large\", weight=\"bold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7301c36-d436-42fc-92fc-c092f7c29877",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Dropping the table because we may have updated the Dataframe\n",
    "\n",
    "DROP TABLE IF EXISTS main.solution_accelerator.eeg_rest_ref_silver;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb56d757-b0ca-43f7-9d32-779180072146",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### `Export` REST Data and Create Silver Layer Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa100bda-42cc-4fa8-a4d3-a89c671d1539",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export REST Data to Silver Table\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "created = False\n",
    "\n",
    "for pt in mne_rest_all:\n",
    "    print(pt)\n",
    "    mne_rest_pt = mne_rest_all[pt]\n",
    "    # print(f\"TYPE::{type(mne_rest_pt)}\")\n",
    "    df_rest_pd = mne_rest_pt.to_data_frame(picks=[\"eeg\"])\n",
    "    # print(f\"PT::{pt} LEN:{len(df_rest_pd.index)}\")\n",
    "    # Add a new index column. We do this so the data goes in, in order for the sine waves\n",
    "    df_rest_pd['index_id'] = df_rest_pd.index\n",
    "    df_rest_spark = spark.createDataFrame(df_rest_pd)\n",
    "    df_rest_spark = df_rest_spark.withColumn('patient_id', lit(pt))\n",
    "    # Establish a persistent delta table by converting the Spark DataFrames into a Delta Table\n",
    "    if not created:\n",
    "        # Replace any previously existing table (\"overwrite\") and register the Spark DataFrames as a Delta table in Catalog\n",
    "        print(\"CREATE\")\n",
    "        df_rest_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.solution_accelerator.eeg_rest_ref_silver\")\n",
    "        created = True\n",
    "    else:\n",
    "        print(\"APPEND\")\n",
    "        df_rest_spark.write.format(\"delta\").mode(\"append\").saveAsTable(\"main.solution_accelerator.eeg_rest_ref_silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65956472-b437-475c-ad04-2034a9d9c0cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Examine statistical metrics utilizing Databricks' integrated commands `describe` and `summary` for potential future data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd115c19-e534-4383-a623-2f364db2fc1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_rest_master.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817df569-f162-4237-8b7c-1189cb50cf59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_rest_master.summary()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": null,
       "elementNUID": "dbe1e1c9-0591-4e76-84d2-189fba58b141",
       "elementType": "command",
       "guid": "2d8db3ff-c0fd-4add-8a89-9eee45299a7a",
       "options": null,
       "position": {
        "height": 9,
        "width": 13,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "ddb5a3b4-a1e8-4a54-aa9a-887437b11eba",
     "origId": 141866486724391,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 536507065719581,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03.1_data_inspect_create_reference_and_silver_tables",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
