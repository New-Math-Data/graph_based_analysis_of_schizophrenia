# Databricks notebook source
# MAGIC %md
# MAGIC ### Overview - Data Inspection and Create Silver Table 
# MAGIC
# MAGIC While EEG recordings can be made without a reference electrode, the use of a reference electrode is essential for accurate and meaningful EEG analysis. It helps in grounding the electrical potentials, canceling out common-mode noise, facilitating signal comparison, and enabling various analytical techniques. The choice of reference scheme should be made based on the experimental requirements and analytical considerations.
# MAGIC
# MAGIC ##### In this notebook we will:
# MAGIC   * Contrast the different techniques used in EEG data analysis to re-reference the data.
# MAGIC     * Zero Reference Method
# MAGIC       - In this method, you choose one electrode to be the reference and subtract its signal from all other electrodes.
# MAGIC       - Each patient has distinct noise and artifact frequencies that are not part of the usable data and must be identified and filtered out.
# MAGIC     * Average Reference Method
# MAGIC       - In this method, you calculate the average signal across all electrodes and subtract this average from each electrode. This method assumes that the average potential of all electrodes represents a good approximation of zero potential.
# MAGIC   * Explore dataset based on summarize statistics. 
# MAGIC   * Examine statistical metrics utilizing Databricks' integrated commands `describe` and `summary` for potential future data manipulation.
# MAGIC     * `describe`: Provides statistics including count, mean, standard deviation, minimum, and maximum.
# MAGIC     * `summary`: describe + interquartile range (IQR)
# MAGIC     * Look for missing or null values in the data that will cause outliers or skew the data, zero values are expected with EEG data so no need to wrangle them.
# MAGIC   * Create Silver Layer Table

# COMMAND ----------

# MAGIC %md
# MAGIC ###### Retrieve data from Bronze Table

# COMMAND ----------

df_bronze_control = spark.sql("""SELECT * FROM main.solution_accelerator.eeg_data_bronze_control""")
df_bronze_study = spark.sql("""SELECT * FROM main.solution_accelerator.eeg_data_bronze_study""")

# Union the DataFrames
df_bronze_patients = df_bronze_control.union(df_bronze_study)

# Show the result
display(df_bronze_patients)
# df_bronze_patients.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ###### Convert PySpark Dataframe to Pandas Dataframe

# COMMAND ----------

# Convert our two PySpark Dataframes to Pandas Dataframes
display(df_bronze_patients.head())

df_patients = df_bronze_patients.toPandas()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Contrast the different techniques used in EEG data analysis to re-reference the data.
# MAGIC
# MAGIC Both the Zero Reference (ZR) and Average Reference (AR) methods are used in EEG (Electroencephalography) to mitigate common noise sources and spatial biases. However, they differ in their approach to reference point selection and signal processing. 
# MAGIC
# MAGIC Contrast the EEG reference points generated by employing both the ZR and AR methods to determine the perferred EEG reference point for our specific use case.

# COMMAND ----------

# MAGIC %md
# MAGIC ###### Zero Reference Method
# MAGIC In this method, you choose one electrode to be the reference and subtract its signal from all other electrodes.

# COMMAND ----------

# Helper library with many built-in functions
%pip install mne

# COMMAND ----------

import os
import mne
import pandas as pd
# import numpy as np
from pyspark.sql.functions import lit

####### CALCULATING THE REFERENCE ELECTRODE USING THE ZERO REFERENCE METHOD #######

# Make a copy of the Dataframe because we are going to apply two different methods and we dont want to override the original
df_zero_ref_cp = df_patients.copy()

# TODO: Perform the following steps for each patient

filtered_data = {}
for col in df_zero_ref_cp.head():
    if col != 'patient_id':
        filtered_data[col] = df_zero_ref_cp.values
    else:
        # Keep the excluded column as is
        filtered_data[col] = df_zero_ref_cp[col].values 

# Get channel values only, extract patient_id column. Numpy will remove the index and column labels
df_numeric = df_zero_ref_cp.select_dtypes(include=[np.number])

# Convert DataFrame to NumPy array and extract the values and transpose it (channels x samples)
np_samples_array = df_numeric.values.T

# Sampling rate in Hz
sfreq = 250

# Get channel names , extract patient_id column
ch_names = [c for c in df_zero_ref_cp.columns if c != 'patient_id']
print(f"ref_channels:::{ch_names}")

# Create an info structure needed by MNE
info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')

# Create the MNE Raw object
mne_raw = mne.io.RawArray(filtered_data[col], info)

# Now we have our MNE Raw object and are ready for further analysis

# Specify the reference channels, extract columns 'Fz' and 'Cz'
ref_channels = [c for c in df_zero_ref_cp.columns if c in ['Fz', 'Cz']]

# Compute the average reference signal from Fz and Cz and apply it
mne_raw.set_eeg_reference(ref_channels=ref_channels)

# Extract EEG data from mne_raw and make a times column
eeg_data, data_time = mne_raw.get_data(return_times=True)
print(eeg_data.shape)
print(data_time.shape)

# Create DataFrame
df_zero_ref = pd.DataFrame(data=eeg_data.T, columns=ch_names)

# Add a new column
df_zero_ref['data_time'] = data_time


# COMMAND ----------

# Output the Dataframe Schema 
df_zero_ref.printSchema()

# Checking number of rows in Dataframes
rows_all = df_zero_ref.count()
print(f"Number of N rows {rows_all}")

# COMMAND ----------

# MAGIC %md
# MAGIC ###### Average Reference Method
# MAGIC This method re-references each electrode to the average potential of all electrodes.

# COMMAND ----------

# Install required libraries
%pip install pyEDFlib

# COMMAND ----------

import os
import mne
import pandas as pd
import pyedflib
import numpy as np
from pyspark.sql.functions import lit

# Directory path
directory = '/tmp'

# Iterate over files in the directory
for filename in os.listdir(directory):
    print(filename)
    # Path to the EEG file (EDF format file in DBFS)
    if os.path.isfile(os.path.join(directory, filename)):
        if "edf" in filename:
            # Create the file path
            filepath = os.path.join(directory, filename)

            # Read the EDF file
            edf_f = pyedflib.EdfReader(filepath)

            # Extract data from the .edf file

            # Get the number of signals, i.e. number of electrode locations
            n_signals = edf_f.signals_in_file

            # Get the labels of each signal, i.e. electrode location names
            signal_labels = edf_f.getSignalLabels()
            
            print(f"signal_labels:::{signal_labels}")

            # Create a dictionary to hold the data
            signal_data = {}

            # Read each signal into the dictionary
            for i in range(n_signals):
                signal_data[signal_labels[i]] = edf_f.readSignal(i)
                # Verify we have 250 Hz of datapoints, 15 min * 60 sec = 900, 900 * 250 = 225000
                print(f"Num of Signals:::{signal_labels[i]} Num{len( signal_data[signal_labels[i]])}")
            print(signal_data.keys())

            # Close the .edf file
            edf_f._close()
            del edf_f

            # making a Pandas DataFrame first is faster and will ensure the double data type for the signals 
            df_eeg_data = pd.DataFrame.from_dict(signal_data)

            ####### CALCULATING THE REFERENCE ELECTRODE USING THE AVERAGE REFERENCE METHOD #######

            # Calculate the average signal across all channels
            average_reference = np.mean(df_eeg_data, axis=0)

            # Subtract the average reference from each channel
            eeg_data_average_referenced = df_eeg_data - average_reference

            # eeg_data_average_referenced is now re-referenced to the average reference

            fn = filename.replace(".edf","")
            print(f"FN::{fn}")

            df_spark = spark.createDataFrame(eeg_data_average_referenced)
            df_spark = df_spark.withColumn('patient_id', lit(fn))
     
            pt = "h"
            if fn.startswith("s"):
                pt = "s"
            df_avg_ref_subjects_all = df_spark.withColumn('subject', lit(pt))

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Average Reference Method

# COMMAND ----------

import mne

####### CALCULATING THE REFERENCE ELECTRODE USING THE AVERAGE REFERENCE METHOD #######

# Make a copy of the Dataframe because we are going to apply two different methods and we dont want to override the original
df_avg_ref_cp = df_avg_ref_all.copy()

raw = mne.io.read_raw_fif(df_avg_ref_cp, preload=True)

# Apply average reference
df_avg_ref_cp = raw.copy().set_eeg_reference(ref_channels='average')

# Plot the average referenced data
df_avg_ref_cp.plot(n_channels=10, title='Average Referenced Data', show=True)


# COMMAND ----------

# Output the Dataframe Schema 
df_avg_ref_cp.printSchema()

# Checking number of rows in Dataframes
rows_sub_all = df_avg_ref_cp.count()
print(f"Number of N rows {rows_sub_all}")

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Explore dataset based on `summarize` statistics

# COMMAND ----------

dbutils.data.summarize(df_zero_ref)
dbutils.data.summarize(df_avg_ref_cp)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Examine statistical metrics utilizing Databricks' integrated commands `describe` and `summary` for potential future data manipulation.

# COMMAND ----------

df_zero_ref.describe()
df_avg_ref_cp.describe()

# COMMAND ----------

df_zero_ref.summary()
df_avg_ref_cp.summary()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Create Silver Layer Table

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Dropping the table because we may have updated the Dataframe
# MAGIC
# MAGIC DROP TABLE IF EXISTS main.solution_accelerator.eeg_zero_ref_silver;
# MAGIC DROP TABLE IF EXISTS main.solution_accelerator.eeg_avg_ref_silver;

# COMMAND ----------

# Structure and integrate raw data into a Delta table for analysis

# Establish a persistent delta table by converting the previously created Spark DataFrames into a Delta Tables.

# Replace any previously existing table and register the DataFrame as a Delta table in the metastore.
df_zero_ref.write.format("delta").mode("overwrite").saveAsTable("main.solution_accelerator.eeg_zero_ref_silver")
df_avg_ref_cp.write.format("delta").mode("overwrite").saveAsTable("main.solution_accelerator.eeg_avg_ref_silver")

print("Tables exist.")

# Delete the Dataframes from memory
del df_zero_ref_all
del df_zero_ref_cp
del df_zero_ref

del df_avg_ref_all
del df_avg_ref_cp

